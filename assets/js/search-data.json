{
  
    
        "post0": {
            "title": "Segmentación",
            "content": ". import numpy as np import matplotlib.pyplot as plt import tensorflow as tf import tensorflow_datasets as tfds from tensorflow.keras.utils import plot_model . Carga de los datos . Para poder centrarnos en la parte interesante del ejercicio utilizaremos el dataset que se incluye en tensorflow_datasets. Podemos cargarlo de la siguiente manera: . dataset, info = tfds.load(&#39;oxford_iiit_pet:3.*.*&#39;, with_info=True, data_dir=&quot;./data/&quot;) . Si no específicamos ningún valor para el parámetro split al utilizar .load(), dataset es un diccionario que contiene tanto el dataset de entrenamiento como el de test: . print(dataset.keys()) dataset[&#39;train&#39;] . dict_keys([&#39;test&#39;, &#39;train&#39;]) . &lt;PrefetchDataset shapes: {file_name: (), image: (None, None, 3), label: (), segmentation_mask: (None, None, 1), species: ()}, types: {file_name: tf.string, image: tf.uint8, label: tf.int64, segmentation_mask: tf.uint8, species: tf.int64}&gt; . También podemos ver la cantidad de elementos que hay que cada conjunto de datos. Vemos que están prácticamente repartidos por igual: . print(f&quot;Ejemplos de entrenamiento: {info.splits[&#39;train&#39;].num_examples}&quot;) print(f&quot;Ejemplos de test: {info.splits[&#39;test&#39;].num_examples}&quot;) . Ejemplos de entrenamiento: 3680 Ejemplos de test: 3669 . Cada uno de los conjuntos es un objeto de TensorFlow cuyos elementos también son diccionarios. Cada elemento contiene la ruta de la imágen a cargar, la propia imagen, la máscara de segmentación correspondiente y la etiqueta. Podemos explorar uno de estos ejemplos para ver cómo son las imágenes y las máscaras: . for a in dataset[&#39;train&#39;]: break . a.keys() . dict_keys([&#39;file_name&#39;, &#39;image&#39;, &#39;label&#39;, &#39;segmentation_mask&#39;, &#39;species&#39;]) . fig, axes = plt.subplots(1,2) axes[0].imshow(a[&#39;image&#39;].numpy().squeeze()) axes[0].set_title(&#39;Original Image&#39;) axes[0].axis(&#39;off&#39;) axes[1].imshow(a[&#39;segmentation_mask&#39;].numpy().squeeze()) axes[1].set_title(&#39;Segmentation Mask&#39;) axes[1].axis(&#39;off&#39;) plt.show() . Es importante darse cuenta de que las imágenes están en el rango $[0,255]$ y de que las etiquetas de las máscaras son $ {1,2,3 }$, ya que es crucial para el entrenamiento de nuestro modelo. Tenemos que normalizar las imágenes para que queden en el rango $[0,1]$ y restar 1 a las etiquetas de las máscaras para que sean $ {0,1,2 }$. . La normalización es importante para que nuestra red pueda aprender, mientras que el cambio en las etiquetas se hace por convención, ya que las funciones de coste esperan recibir así las etiquetas. . print(&quot;Nuestras imágenes están en el rango [{},{}] y tienen dimensiones ({}).&quot;.format(a[&#39;image&#39;].numpy().min(), a[&#39;image&#39;].numpy().max(), a[&#39;image&#39;].numpy().shape)) print(&quot;Nuestras etiquetas son {}.&quot;.format(np.unique(a[&#39;segmentation_mask&#39;].numpy()))) . Nuestras imágenes están en el rango [0,255] y tienen dimensiones ((500, 500, 3)). Nuestras etiquetas son [1 2 3]. . Normalizaci&#243;n de las im&#225;genes y ajuste de las etiquetas en las m&#225;scaras de segmentaci&#243;n. . Dado que estamos trabajando con un Dataset de TensorFlow, la forma más cómoda de aplicar una transformación sobre le dataset completo es utilizando el método .map(), que permite aplicar una función a todos los elementos del conjunto y nos devuelve un nuevo Dataset con los resultados. Para ello crearemos una función de normalización y la aplicaremos utilizando .map(): . def normalize(sample, reshape_dims=(128,128)): ## Extraemos la imagen y la máscara de segmentación porque ## el resto de información no nos interesa. image = sample[&#39;image&#39;] segmentation_mask = sample[&#39;segmentation_mask&#39;] ## Aprovechamos para hacer más pequeñas las imágenes, ya que ## 500x500 es demasiado grande para un ejemplo. ## Para la máscara se utiliza el método nearest para evitar ## obtener números intermedios. image = tf.image.resize(image, reshape_dims) segmentation_mask = tf.image.resize(segmentation_mask, reshape_dims, method=&#39;nearest&#39;) ## Normalizamos las imágenes y ajustamos las etiquetas image = image/255 segmentation_mask = segmentation_mask-1 return image, segmentation_mask . train = dataset[&#39;train&#39;].map(normalize) test = dataset[&#39;test&#39;].map(normalize) . for a in train: break . plt.figure() plt.imshow(a[1].numpy().squeeze()) plt.show() . plt.hist(np.round(a[1].numpy().ravel())) plt.show() . print(&quot;Nuestras imágenes están en el rango [{},{}].&quot;.format(a[0].numpy().min(), a[0].numpy().max(), a[0].numpy().shape)) print(&quot;Nuestras etiquetas son {}.&quot;.format(np.unique(a[1].numpy()))) . Nuestras imágenes están en el rango [0.0,0.7918505072593689]. Nuestras etiquetas son [0 1 2]. . Definici&#243;n del modelo . El modelo por excelencia para resolver problemas de segmentación es la U-Net. Estos modelos se basan en una estructura encoder-decoder que toma una imagen como entrada y devuelve una imagen que tiene tantos canales como clases diferentes a predecir. Esta parte es clave, ya que es lo que nos permite plantear un problema de clasificación por pixel. . Si nuestra imagen de salida tiene tantos canales como clases tenemos, solamente tendremos que aplicar una función softmax en la dirección de los canales para obtener el canal (o la clase) más probable para cada pixel. . Veamos como plantearlo: . model = tf.keras.models.Sequential([ tf.keras.layers.Conv2D(32, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(128,128,3)), tf.keras.layers.Conv2D(32, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;), tf.keras.layers.MaxPool2D(2), tf.keras.layers.Conv2D(64, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;), tf.keras.layers.MaxPool2D(2), tf.keras.layers.Conv2D(128, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;), tf.keras.layers.MaxPool2D(2), tf.keras.layers.Conv2DTranspose(64, kernel_size=3, padding=&#39;same&#39;, strides=2, activation=&#39;relu&#39;), tf.keras.layers.Conv2DTranspose(32, kernel_size=3, padding=&#39;same&#39;, strides=2, activation=&#39;relu&#39;), tf.keras.layers.Conv2DTranspose(3, kernel_size=3, padding=&#39;same&#39;, strides=2, activation=&#39;linear&#39;), ]) model.summary() model.compile(optimizer=&#39;adam&#39;, loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) . Model: &#34;sequential&#34; _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= conv2d (Conv2D) (None, 128, 128, 32) 896 _________________________________________________________________ conv2d_1 (Conv2D) (None, 128, 128, 32) 9248 _________________________________________________________________ max_pooling2d (MaxPooling2D) (None, 64, 64, 32) 0 _________________________________________________________________ conv2d_2 (Conv2D) (None, 64, 64, 64) 18496 _________________________________________________________________ max_pooling2d_1 (MaxPooling2 (None, 32, 32, 64) 0 _________________________________________________________________ conv2d_3 (Conv2D) (None, 32, 32, 128) 73856 _________________________________________________________________ max_pooling2d_2 (MaxPooling2 (None, 16, 16, 128) 0 _________________________________________________________________ conv2d_transpose (Conv2DTran (None, 32, 32, 64) 73792 _________________________________________________________________ conv2d_transpose_1 (Conv2DTr (None, 64, 64, 32) 18464 _________________________________________________________________ conv2d_transpose_2 (Conv2DTr (None, 128, 128, 3) 867 ================================================================= Total params: 195,619 Trainable params: 195,619 Non-trainable params: 0 _________________________________________________________________ . La funcion tensorflow.keras.utils.plot_model() nos permite generar una imagen de la estructura de nuestra red. Para que funcione es necesario tener instalados pydot y graphviz, pero podemos instalar fácilmente los dos paquetes con conda (conda install pydot graphviz). . plot_model(model, to_file=&quot;cnn_segmentation.png&quot;, show_shapes=True) . history = model.fit(train.batch(32), epochs=100, validation_data=test.batch(32)) . Epoch 1/100 115/115 [==============================] - 31s 234ms/step - loss: 0.8881 - accuracy: 0.5848 - val_loss: 0.9062 - val_accuracy: 0.5788 Epoch 2/100 115/115 [==============================] - 26s 230ms/step - loss: 0.8106 - accuracy: 0.6111 - val_loss: 0.8085 - val_accuracy: 0.6210 Epoch 3/100 115/115 [==============================] - 26s 229ms/step - loss: 0.7885 - accuracy: 0.6283 - val_loss: 0.7664 - val_accuracy: 0.6612 Epoch 4/100 115/115 [==============================] - 26s 230ms/step - loss: 0.7384 - accuracy: 0.6773 - val_loss: 0.7468 - val_accuracy: 0.6637 Epoch 5/100 115/115 [==============================] - 26s 229ms/step - loss: 0.6878 - accuracy: 0.7031 - val_loss: 0.6609 - val_accuracy: 0.7160 Epoch 6/100 115/115 [==============================] - 26s 230ms/step - loss: 0.6444 - accuracy: 0.7261 - val_loss: 0.6313 - val_accuracy: 0.7361 Epoch 7/100 115/115 [==============================] - 27s 232ms/step - loss: 0.6241 - accuracy: 0.7362 - val_loss: 0.6114 - val_accuracy: 0.7482 Epoch 8/100 115/115 [==============================] - 27s 233ms/step - loss: 0.6046 - accuracy: 0.7462 - val_loss: 0.5897 - val_accuracy: 0.7580 Epoch 9/100 115/115 [==============================] - 27s 231ms/step - loss: 0.5852 - accuracy: 0.7562 - val_loss: 0.5871 - val_accuracy: 0.7610 Epoch 10/100 115/115 [==============================] - 27s 231ms/step - loss: 0.5763 - accuracy: 0.7608 - val_loss: 0.5669 - val_accuracy: 0.7685 Epoch 11/100 115/115 [==============================] - 26s 230ms/step - loss: 0.5625 - accuracy: 0.7675 - val_loss: 0.5639 - val_accuracy: 0.7685 Epoch 12/100 115/115 [==============================] - 27s 231ms/step - loss: 0.5550 - accuracy: 0.7711 - val_loss: 0.5583 - val_accuracy: 0.7713 Epoch 13/100 115/115 [==============================] - 27s 231ms/step - loss: 0.5461 - accuracy: 0.7753 - val_loss: 0.5629 - val_accuracy: 0.7664 Epoch 14/100 115/115 [==============================] - 27s 231ms/step - loss: 0.5411 - accuracy: 0.7775 - val_loss: 0.5396 - val_accuracy: 0.7779 Epoch 15/100 115/115 [==============================] - 26s 230ms/step - loss: 0.5338 - accuracy: 0.7809 - val_loss: 0.5460 - val_accuracy: 0.7743 Epoch 16/100 115/115 [==============================] - 27s 232ms/step - loss: 0.5286 - accuracy: 0.7834 - val_loss: 0.5366 - val_accuracy: 0.7793 Epoch 17/100 115/115 [==============================] - 27s 231ms/step - loss: 0.5280 - accuracy: 0.7839 - val_loss: 0.5081 - val_accuracy: 0.7942 Epoch 18/100 115/115 [==============================] - 27s 231ms/step - loss: 0.5194 - accuracy: 0.7880 - val_loss: 0.5070 - val_accuracy: 0.7942 Epoch 19/100 115/115 [==============================] - 27s 231ms/step - loss: 0.5171 - accuracy: 0.7891 - val_loss: 0.5114 - val_accuracy: 0.7930 Epoch 20/100 115/115 [==============================] - 26s 230ms/step - loss: 0.5108 - accuracy: 0.7920 - val_loss: 0.5155 - val_accuracy: 0.7893 Epoch 21/100 115/115 [==============================] - 26s 231ms/step - loss: 0.5050 - accuracy: 0.7948 - val_loss: 0.5044 - val_accuracy: 0.7955 Epoch 22/100 115/115 [==============================] - 27s 231ms/step - loss: 0.5000 - accuracy: 0.7971 - val_loss: 0.5032 - val_accuracy: 0.7956 Epoch 23/100 115/115 [==============================] - 26s 229ms/step - loss: 0.4940 - accuracy: 0.7996 - val_loss: 0.5000 - val_accuracy: 0.7972 Epoch 24/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4948 - accuracy: 0.7994 - val_loss: 0.4846 - val_accuracy: 0.8050 Epoch 25/100 115/115 [==============================] - 26s 229ms/step - loss: 0.4928 - accuracy: 0.8003 - val_loss: 0.4821 - val_accuracy: 0.8066 Epoch 26/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4875 - accuracy: 0.8026 - val_loss: 0.4914 - val_accuracy: 0.8018 Epoch 27/100 115/115 [==============================] - 26s 228ms/step - loss: 0.4841 - accuracy: 0.8040 - val_loss: 0.4775 - val_accuracy: 0.8085 Epoch 28/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4816 - accuracy: 0.8049 - val_loss: 0.4789 - val_accuracy: 0.8075 Epoch 29/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4764 - accuracy: 0.8074 - val_loss: 0.4770 - val_accuracy: 0.8084 Epoch 30/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4710 - accuracy: 0.8101 - val_loss: 0.4754 - val_accuracy: 0.8096 Epoch 31/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4715 - accuracy: 0.8098 - val_loss: 0.4731 - val_accuracy: 0.8104 Epoch 32/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4650 - accuracy: 0.8124 - val_loss: 0.4736 - val_accuracy: 0.8104 Epoch 33/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4630 - accuracy: 0.8135 - val_loss: 0.4757 - val_accuracy: 0.8093 Epoch 34/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4596 - accuracy: 0.8147 - val_loss: 0.4610 - val_accuracy: 0.8165 Epoch 35/100 115/115 [==============================] - 26s 231ms/step - loss: 0.4559 - accuracy: 0.8165 - val_loss: 0.4644 - val_accuracy: 0.8154 Epoch 36/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4531 - accuracy: 0.8178 - val_loss: 0.4622 - val_accuracy: 0.8162 Epoch 37/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4464 - accuracy: 0.8206 - val_loss: 0.4547 - val_accuracy: 0.8193 Epoch 38/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4449 - accuracy: 0.8213 - val_loss: 0.4528 - val_accuracy: 0.8202 Epoch 39/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4405 - accuracy: 0.8232 - val_loss: 0.4517 - val_accuracy: 0.8206 Epoch 40/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4387 - accuracy: 0.8240 - val_loss: 0.4444 - val_accuracy: 0.8240 Epoch 41/100 115/115 [==============================] - 27s 232ms/step - loss: 0.4365 - accuracy: 0.8250 - val_loss: 0.4478 - val_accuracy: 0.8232 Epoch 42/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4354 - accuracy: 0.8255 - val_loss: 0.4488 - val_accuracy: 0.8225 Epoch 43/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4321 - accuracy: 0.8267 - val_loss: 0.4535 - val_accuracy: 0.8208 Epoch 44/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4285 - accuracy: 0.8284 - val_loss: 0.4494 - val_accuracy: 0.8223 Epoch 45/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4254 - accuracy: 0.8297 - val_loss: 0.4428 - val_accuracy: 0.8248 Epoch 46/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4236 - accuracy: 0.8304 - val_loss: 0.4422 - val_accuracy: 0.8249 Epoch 47/100 115/115 [==============================] - 27s 232ms/step - loss: 0.4226 - accuracy: 0.8308 - val_loss: 0.4429 - val_accuracy: 0.8248 Epoch 48/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4209 - accuracy: 0.8316 - val_loss: 0.4499 - val_accuracy: 0.8227 Epoch 49/100 115/115 [==============================] - 27s 232ms/step - loss: 0.4175 - accuracy: 0.8331 - val_loss: 0.4501 - val_accuracy: 0.8228 Epoch 50/100 115/115 [==============================] - 27s 234ms/step - loss: 0.4165 - accuracy: 0.8332 - val_loss: 0.4483 - val_accuracy: 0.8231 Epoch 51/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4137 - accuracy: 0.8348 - val_loss: 0.4493 - val_accuracy: 0.8227 Epoch 52/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4130 - accuracy: 0.8349 - val_loss: 0.4445 - val_accuracy: 0.8246 Epoch 53/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4101 - accuracy: 0.8362 - val_loss: 0.4441 - val_accuracy: 0.8251 Epoch 54/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4082 - accuracy: 0.8370 - val_loss: 0.4446 - val_accuracy: 0.8250 Epoch 55/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4069 - accuracy: 0.8374 - val_loss: 0.4409 - val_accuracy: 0.8263 Epoch 56/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4063 - accuracy: 0.8376 - val_loss: 0.4461 - val_accuracy: 0.8239 Epoch 57/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4063 - accuracy: 0.8376 - val_loss: 0.4442 - val_accuracy: 0.8253 Epoch 58/100 115/115 [==============================] - 27s 232ms/step - loss: 0.4069 - accuracy: 0.8373 - val_loss: 0.4406 - val_accuracy: 0.8271 Epoch 59/100 115/115 [==============================] - 26s 230ms/step - loss: 0.4069 - accuracy: 0.8374 - val_loss: 0.4546 - val_accuracy: 0.8221 Epoch 60/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4019 - accuracy: 0.8395 - val_loss: 0.4373 - val_accuracy: 0.8281 Epoch 61/100 115/115 [==============================] - 27s 233ms/step - loss: 0.4053 - accuracy: 0.8380 - val_loss: 0.4376 - val_accuracy: 0.8283 Epoch 62/100 115/115 [==============================] - 27s 231ms/step - loss: 0.4010 - accuracy: 0.8397 - val_loss: 0.4327 - val_accuracy: 0.8301 Epoch 63/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3982 - accuracy: 0.8411 - val_loss: 0.4320 - val_accuracy: 0.8303 Epoch 64/100 115/115 [==============================] - 26s 230ms/step - loss: 0.3964 - accuracy: 0.8419 - val_loss: 0.4317 - val_accuracy: 0.8307 Epoch 65/100 115/115 [==============================] - 26s 230ms/step - loss: 0.3944 - accuracy: 0.8426 - val_loss: 0.4281 - val_accuracy: 0.8321 Epoch 66/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3912 - accuracy: 0.8440 - val_loss: 0.4344 - val_accuracy: 0.8299 Epoch 67/100 115/115 [==============================] - 27s 231ms/step - loss: 0.3896 - accuracy: 0.8445 - val_loss: 0.4281 - val_accuracy: 0.8322 Epoch 68/100 115/115 [==============================] - 27s 231ms/step - loss: 0.3865 - accuracy: 0.8458 - val_loss: 0.4302 - val_accuracy: 0.8318 Epoch 69/100 115/115 [==============================] - 27s 234ms/step - loss: 0.3845 - accuracy: 0.8467 - val_loss: 0.4366 - val_accuracy: 0.8292 Epoch 70/100 115/115 [==============================] - 27s 231ms/step - loss: 0.3859 - accuracy: 0.8460 - val_loss: 0.4347 - val_accuracy: 0.8305 Epoch 71/100 115/115 [==============================] - 26s 228ms/step - loss: 0.3874 - accuracy: 0.8456 - val_loss: 0.4294 - val_accuracy: 0.8321 Epoch 72/100 115/115 [==============================] - 27s 231ms/step - loss: 0.3850 - accuracy: 0.8466 - val_loss: 0.4313 - val_accuracy: 0.8315 Epoch 73/100 115/115 [==============================] - 27s 233ms/step - loss: 0.3834 - accuracy: 0.8472 - val_loss: 0.4427 - val_accuracy: 0.8283 Epoch 74/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3845 - accuracy: 0.8466 - val_loss: 0.4359 - val_accuracy: 0.8308 Epoch 75/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3820 - accuracy: 0.8478 - val_loss: 0.4404 - val_accuracy: 0.8292 Epoch 76/100 115/115 [==============================] - 27s 233ms/step - loss: 0.3796 - accuracy: 0.8486 - val_loss: 0.4316 - val_accuracy: 0.8313 Epoch 77/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3764 - accuracy: 0.8500 - val_loss: 0.4303 - val_accuracy: 0.8325 Epoch 78/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3757 - accuracy: 0.8502 - val_loss: 0.4372 - val_accuracy: 0.8304 Epoch 79/100 115/115 [==============================] - 26s 231ms/step - loss: 0.3759 - accuracy: 0.8502 - val_loss: 0.4408 - val_accuracy: 0.8294 Epoch 80/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3763 - accuracy: 0.8500 - val_loss: 0.4389 - val_accuracy: 0.8305 Epoch 81/100 115/115 [==============================] - 26s 231ms/step - loss: 0.3755 - accuracy: 0.8503 - val_loss: 0.4435 - val_accuracy: 0.8292 Epoch 82/100 115/115 [==============================] - 27s 233ms/step - loss: 0.3754 - accuracy: 0.8503 - val_loss: 0.4452 - val_accuracy: 0.8281 Epoch 83/100 115/115 [==============================] - 26s 230ms/step - loss: 0.3778 - accuracy: 0.8493 - val_loss: 0.4418 - val_accuracy: 0.8297 Epoch 84/100 115/115 [==============================] - 26s 230ms/step - loss: 0.3753 - accuracy: 0.8504 - val_loss: 0.4497 - val_accuracy: 0.8275 Epoch 85/100 115/115 [==============================] - 26s 230ms/step - loss: 0.3807 - accuracy: 0.8481 - val_loss: 0.4683 - val_accuracy: 0.8197 Epoch 86/100 115/115 [==============================] - 26s 229ms/step - loss: 0.3795 - accuracy: 0.8487 - val_loss: 0.4651 - val_accuracy: 0.8214 Epoch 87/100 115/115 [==============================] - 26s 229ms/step - loss: 0.3743 - accuracy: 0.8508 - val_loss: 0.4549 - val_accuracy: 0.8256 Epoch 88/100 115/115 [==============================] - 26s 228ms/step - loss: 0.3716 - accuracy: 0.8520 - val_loss: 0.4674 - val_accuracy: 0.8208 Epoch 89/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3742 - accuracy: 0.8508 - val_loss: 0.4588 - val_accuracy: 0.8245 Epoch 90/100 115/115 [==============================] - 26s 230ms/step - loss: 0.3714 - accuracy: 0.8521 - val_loss: 0.4522 - val_accuracy: 0.8270 Epoch 91/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3693 - accuracy: 0.8529 - val_loss: 0.4507 - val_accuracy: 0.8272 Epoch 92/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3679 - accuracy: 0.8535 - val_loss: 0.4544 - val_accuracy: 0.8268 Epoch 93/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3656 - accuracy: 0.8545 - val_loss: 0.4636 - val_accuracy: 0.8240 Epoch 94/100 115/115 [==============================] - 27s 231ms/step - loss: 0.3658 - accuracy: 0.8542 - val_loss: 0.4628 - val_accuracy: 0.8249 Epoch 95/100 115/115 [==============================] - 26s 230ms/step - loss: 0.3675 - accuracy: 0.8535 - val_loss: 0.4590 - val_accuracy: 0.8264 Epoch 96/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3644 - accuracy: 0.8548 - val_loss: 0.4673 - val_accuracy: 0.8236 Epoch 97/100 115/115 [==============================] - 27s 231ms/step - loss: 0.3641 - accuracy: 0.8548 - val_loss: 0.4701 - val_accuracy: 0.8232 Epoch 98/100 115/115 [==============================] - 26s 230ms/step - loss: 0.3658 - accuracy: 0.8542 - val_loss: 0.4654 - val_accuracy: 0.8246 Epoch 99/100 115/115 [==============================] - 27s 232ms/step - loss: 0.3635 - accuracy: 0.8549 - val_loss: 0.4739 - val_accuracy: 0.8212 Epoch 100/100 115/115 [==============================] - 27s 231ms/step - loss: 0.3671 - accuracy: 0.8535 - val_loss: 0.4585 - val_accuracy: 0.8258 . for a,b in train.batch(1): pred = model(a) break . fig, axes = plt.subplots(1,2) axes[0].imshow(b.numpy().squeeze()) axes[0].set_title(&#39;Original Mask&#39;) axes[0].axis(&#39;off&#39;) axes[1].imshow(np.argmax(pred.numpy().squeeze(),-1)) axes[1].set_title(&#39;Predicted Mask&#39;) axes[1].axis(&#39;off&#39;) plt.show() . def show_results(model, dataset, n=5): for x, y in dataset.batch(n): pred = model(x) break fig, axes = plt.subplots(2, n) for col in range(n): axes[0,col].imshow(y[col].numpy().squeeze()) axes[0,col].set_title(&#39;Original&#39;) axes[0,col].axis(&#39;off&#39;) axes[1,col].imshow(np.argmax(pred[col].numpy().squeeze(),-1)) axes[1,col].set_title(&#39;Predicted&#39;) axes[1,col].axis(&#39;off&#39;) fig.tight_layout() # plt.show() . show_results(model, train, n=5) plt.suptitle(&quot;Train&quot;) plt.show() . show_results(model, test, n=5) plt.suptitle(&quot;Test&quot;) plt.show() . A&#241;adiendo conexiones entre capas . El resultado que obtenemos no está mal, pero no es todo lo bueno que podríamos esperar. Sí que es verdad que el modelo capta bastante rápido el contorno de las formas, pero falta definir mejor el resultado. Para ello vamos a incluir unas conexiones entre las capas del encoder y del decoder. De esta forma la información se propaga mucho mejor y la red puede aprender más fácilmente. . Como ya podemos intuir, la arquitectura que queremos utilizar ya no es un modelo secuencial, así que tendremos que utilizar o bien la API funcional o la herencia de clases. En este caso optamos por utilizar la API funcional. . Ejercicio:Plantea el mismo modelo utilizando herencia de clases. https://upload.wikimedia.org/wikipedia/commons/2/2b/Example_architecture_of_U-Net_for_producing_k_256-by-256_image_masks_for_a_256-by-256_RGB_image.png . ## Encoder conv_1_1 = tf.keras.layers.Conv2D(32, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;, input_shape=(128,128,3)) conv_1_2 = tf.keras.layers.Conv2D(32, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;) conv_2 = tf.keras.layers.Conv2D(64, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;) conv_3 = tf.keras.layers.Conv2D(128, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;) # maxpool_2 = tf.keras.layers.MaxPool2D(2) ## Decoder conv_1_up = tf.keras.layers.Conv2D(64, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;) conv_2_up = tf.keras.layers.Conv2D(32, kernel_size=3, padding=&#39;same&#39;, activation=&#39;relu&#39;) conv_3_up = tf.keras.layers.Conv2D(3, kernel_size=3, padding=&#39;same&#39;, activation=&#39;linear&#39;) # upsampling_2 = tf.keras.layers.UpSampling2D(2) # Definición del flujo de datos ## Encoding x = tf.keras.Input(shape=(128,128,3)) x_conv_1 = conv_1_1(x) x_conv_1 = conv_1_2(x_conv_1) x_conv_1 = tf.keras.layers.MaxPool2D(2)(x_conv_1) x_conv_2 = conv_2(x_conv_1) x_conv_2 = tf.keras.layers.MaxPool2D(2)(x_conv_2) x_conv_3 = conv_3(x_conv_2) x_conv_3 = tf.keras.layers.MaxPool2D(2)(x_conv_3) ## Decoding + Upsampling x_up_2 = tf.keras.layers.UpSampling2D(2)(x_conv_3) x_up_2 = tf.keras.layers.concatenate((x_conv_2, x_up_2)) x_up_2 = conv_1_up(x_up_2) x_up_1 = tf.keras.layers.UpSampling2D(2)(x_up_2) x_up_1 = tf.keras.layers.concatenate((x_conv_1, x_up_1)) x_up_1 = conv_2_up(x_up_1) x_up = tf.keras.layers.UpSampling2D(2)(x_up_1) x_up = tf.keras.layers.concatenate((x, x_up)) x_up = conv_3_up(x_up) . u_net = tf.keras.Model(x, x_up) u_net.summary() u_net.compile(optimizer=&#39;adam&#39;, loss=tf.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=[&#39;accuracy&#39;]) . Model: &#34;model&#34; __________________________________________________________________________________________________ Layer (type) Output Shape Param # Connected to ================================================================================================== input_1 (InputLayer) [(None, 128, 128, 3) 0 __________________________________________________________________________________________________ conv2d_4 (Conv2D) (None, 128, 128, 32) 896 input_1[0][0] __________________________________________________________________________________________________ conv2d_5 (Conv2D) (None, 128, 128, 32) 9248 conv2d_4[0][0] __________________________________________________________________________________________________ max_pooling2d_3 (MaxPooling2D) (None, 64, 64, 32) 0 conv2d_5[0][0] __________________________________________________________________________________________________ conv2d_6 (Conv2D) (None, 64, 64, 64) 18496 max_pooling2d_3[0][0] __________________________________________________________________________________________________ max_pooling2d_4 (MaxPooling2D) (None, 32, 32, 64) 0 conv2d_6[0][0] __________________________________________________________________________________________________ conv2d_7 (Conv2D) (None, 32, 32, 128) 73856 max_pooling2d_4[0][0] __________________________________________________________________________________________________ max_pooling2d_5 (MaxPooling2D) (None, 16, 16, 128) 0 conv2d_7[0][0] __________________________________________________________________________________________________ up_sampling2d (UpSampling2D) (None, 32, 32, 128) 0 max_pooling2d_5[0][0] __________________________________________________________________________________________________ concatenate (Concatenate) (None, 32, 32, 192) 0 max_pooling2d_4[0][0] up_sampling2d[0][0] __________________________________________________________________________________________________ conv2d_8 (Conv2D) (None, 32, 32, 64) 110656 concatenate[0][0] __________________________________________________________________________________________________ up_sampling2d_1 (UpSampling2D) (None, 64, 64, 64) 0 conv2d_8[0][0] __________________________________________________________________________________________________ concatenate_1 (Concatenate) (None, 64, 64, 96) 0 max_pooling2d_3[0][0] up_sampling2d_1[0][0] __________________________________________________________________________________________________ conv2d_9 (Conv2D) (None, 64, 64, 32) 27680 concatenate_1[0][0] __________________________________________________________________________________________________ up_sampling2d_2 (UpSampling2D) (None, 128, 128, 32) 0 conv2d_9[0][0] __________________________________________________________________________________________________ concatenate_2 (Concatenate) (None, 128, 128, 35) 0 input_1[0][0] up_sampling2d_2[0][0] __________________________________________________________________________________________________ conv2d_10 (Conv2D) (None, 128, 128, 3) 948 concatenate_2[0][0] ================================================================================================== Total params: 241,780 Trainable params: 241,780 Non-trainable params: 0 __________________________________________________________________________________________________ . plot_model(u_net, to_file=&quot;u-net.png&quot;, show_shapes=True) . history_unet = u_net.fit(train.batch(32), epochs=100, validation_data=test.batch(32)) . Epoch 1/100 115/115 [==============================] - 30s 248ms/step - loss: 0.8232 - accuracy: 0.6244 - val_loss: 0.7407 - val_accuracy: 0.6708 Epoch 2/100 115/115 [==============================] - 28s 246ms/step - loss: 0.6662 - accuracy: 0.7145 - val_loss: 0.6298 - val_accuracy: 0.7351 Epoch 3/100 115/115 [==============================] - 28s 246ms/step - loss: 0.6011 - accuracy: 0.7499 - val_loss: 0.5776 - val_accuracy: 0.7623 Epoch 4/100 115/115 [==============================] - 28s 247ms/step - loss: 0.5601 - accuracy: 0.7709 - val_loss: 0.5412 - val_accuracy: 0.7813 Epoch 5/100 115/115 [==============================] - 29s 249ms/step - loss: 0.5341 - accuracy: 0.7833 - val_loss: 0.5083 - val_accuracy: 0.7953 Epoch 6/100 115/115 [==============================] - 29s 248ms/step - loss: 0.5131 - accuracy: 0.7926 - val_loss: 0.4969 - val_accuracy: 0.8004 Epoch 7/100 115/115 [==============================] - 28s 247ms/step - loss: 0.4950 - accuracy: 0.8012 - val_loss: 0.4747 - val_accuracy: 0.8108 Epoch 8/100 115/115 [==============================] - 29s 248ms/step - loss: 0.4817 - accuracy: 0.8070 - val_loss: 0.4728 - val_accuracy: 0.8114 Epoch 9/100 115/115 [==============================] - 28s 245ms/step - loss: 0.4760 - accuracy: 0.8091 - val_loss: 0.4736 - val_accuracy: 0.8110 Epoch 10/100 115/115 [==============================] - 29s 249ms/step - loss: 0.4616 - accuracy: 0.8158 - val_loss: 0.4602 - val_accuracy: 0.8174 Epoch 11/100 115/115 [==============================] - 29s 250ms/step - loss: 0.4549 - accuracy: 0.8189 - val_loss: 0.4838 - val_accuracy: 0.8064 Epoch 12/100 115/115 [==============================] - 28s 247ms/step - loss: 0.4451 - accuracy: 0.8226 - val_loss: 0.4454 - val_accuracy: 0.8232 Epoch 13/100 115/115 [==============================] - 29s 248ms/step - loss: 0.4394 - accuracy: 0.8254 - val_loss: 0.4444 - val_accuracy: 0.8241 Epoch 14/100 115/115 [==============================] - 28s 246ms/step - loss: 0.4367 - accuracy: 0.8263 - val_loss: 0.4479 - val_accuracy: 0.8231 Epoch 15/100 115/115 [==============================] - 28s 247ms/step - loss: 0.4261 - accuracy: 0.8310 - val_loss: 0.4373 - val_accuracy: 0.8280 Epoch 16/100 115/115 [==============================] - 29s 248ms/step - loss: 0.4190 - accuracy: 0.8340 - val_loss: 0.4498 - val_accuracy: 0.8234 Epoch 17/100 115/115 [==============================] - 28s 247ms/step - loss: 0.4144 - accuracy: 0.8361 - val_loss: 0.4417 - val_accuracy: 0.8270 Epoch 18/100 115/115 [==============================] - 29s 248ms/step - loss: 0.4064 - accuracy: 0.8395 - val_loss: 0.4335 - val_accuracy: 0.8305 Epoch 19/100 115/115 [==============================] - 28s 247ms/step - loss: 0.4012 - accuracy: 0.8415 - val_loss: 0.4358 - val_accuracy: 0.8298 Epoch 20/100 115/115 [==============================] - 29s 249ms/step - loss: 0.3975 - accuracy: 0.8430 - val_loss: 0.4272 - val_accuracy: 0.8333 Epoch 21/100 115/115 [==============================] - 28s 248ms/step - loss: 0.3905 - accuracy: 0.8458 - val_loss: 0.4403 - val_accuracy: 0.8290 Epoch 22/100 115/115 [==============================] - 28s 245ms/step - loss: 0.3880 - accuracy: 0.8468 - val_loss: 0.4221 - val_accuracy: 0.8362 Epoch 23/100 115/115 [==============================] - 28s 248ms/step - loss: 0.3823 - accuracy: 0.8492 - val_loss: 0.4172 - val_accuracy: 0.8379 Epoch 24/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3789 - accuracy: 0.8508 - val_loss: 0.4214 - val_accuracy: 0.8371 Epoch 25/100 115/115 [==============================] - 28s 247ms/step - loss: 0.3745 - accuracy: 0.8525 - val_loss: 0.4153 - val_accuracy: 0.8391 Epoch 26/100 115/115 [==============================] - 29s 249ms/step - loss: 0.3708 - accuracy: 0.8538 - val_loss: 0.4226 - val_accuracy: 0.8356 Epoch 27/100 115/115 [==============================] - 28s 245ms/step - loss: 0.3661 - accuracy: 0.8561 - val_loss: 0.4151 - val_accuracy: 0.8394 Epoch 28/100 115/115 [==============================] - 29s 248ms/step - loss: 0.3613 - accuracy: 0.8578 - val_loss: 0.4234 - val_accuracy: 0.8365 Epoch 29/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3568 - accuracy: 0.8596 - val_loss: 0.4282 - val_accuracy: 0.8358 Epoch 30/100 115/115 [==============================] - 28s 244ms/step - loss: 0.3574 - accuracy: 0.8593 - val_loss: 0.4277 - val_accuracy: 0.8358 Epoch 31/100 115/115 [==============================] - 28s 244ms/step - loss: 0.3584 - accuracy: 0.8589 - val_loss: 0.4271 - val_accuracy: 0.8342 Epoch 32/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3555 - accuracy: 0.8601 - val_loss: 0.4148 - val_accuracy: 0.8398 Epoch 33/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3491 - accuracy: 0.8626 - val_loss: 0.4336 - val_accuracy: 0.8330 Epoch 34/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3456 - accuracy: 0.8640 - val_loss: 0.4225 - val_accuracy: 0.8381 Epoch 35/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3444 - accuracy: 0.8643 - val_loss: 0.4324 - val_accuracy: 0.8349 Epoch 36/100 115/115 [==============================] - 29s 248ms/step - loss: 0.3436 - accuracy: 0.8648 - val_loss: 0.4387 - val_accuracy: 0.8342 Epoch 37/100 115/115 [==============================] - 29s 248ms/step - loss: 0.3394 - accuracy: 0.8664 - val_loss: 0.4484 - val_accuracy: 0.8306 Epoch 38/100 115/115 [==============================] - 28s 247ms/step - loss: 0.3376 - accuracy: 0.8671 - val_loss: 0.4442 - val_accuracy: 0.8315 Epoch 39/100 115/115 [==============================] - 29s 248ms/step - loss: 0.3350 - accuracy: 0.8680 - val_loss: 0.4521 - val_accuracy: 0.8291 Epoch 40/100 115/115 [==============================] - 29s 249ms/step - loss: 0.3342 - accuracy: 0.8684 - val_loss: 0.4392 - val_accuracy: 0.8333 Epoch 41/100 115/115 [==============================] - 29s 249ms/step - loss: 0.3327 - accuracy: 0.8689 - val_loss: 0.4365 - val_accuracy: 0.8350 Epoch 42/100 115/115 [==============================] - 29s 250ms/step - loss: 0.3327 - accuracy: 0.8690 - val_loss: 0.4296 - val_accuracy: 0.8371 Epoch 43/100 115/115 [==============================] - 28s 248ms/step - loss: 0.3318 - accuracy: 0.8694 - val_loss: 0.4324 - val_accuracy: 0.8371 Epoch 44/100 115/115 [==============================] - 28s 247ms/step - loss: 0.3280 - accuracy: 0.8708 - val_loss: 0.4233 - val_accuracy: 0.8406 Epoch 45/100 115/115 [==============================] - 28s 245ms/step - loss: 0.3265 - accuracy: 0.8714 - val_loss: 0.4269 - val_accuracy: 0.8404 Epoch 46/100 115/115 [==============================] - 28s 247ms/step - loss: 0.3222 - accuracy: 0.8730 - val_loss: 0.4353 - val_accuracy: 0.8394 Epoch 47/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3180 - accuracy: 0.8747 - val_loss: 0.4433 - val_accuracy: 0.8372 Epoch 48/100 115/115 [==============================] - 28s 247ms/step - loss: 0.3150 - accuracy: 0.8759 - val_loss: 0.4474 - val_accuracy: 0.8373 Epoch 49/100 115/115 [==============================] - 28s 247ms/step - loss: 0.3147 - accuracy: 0.8759 - val_loss: 0.4502 - val_accuracy: 0.8361 Epoch 50/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3249 - accuracy: 0.8714 - val_loss: 0.4339 - val_accuracy: 0.8382 Epoch 51/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3350 - accuracy: 0.8678 - val_loss: 0.4121 - val_accuracy: 0.8450 Epoch 52/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3246 - accuracy: 0.8719 - val_loss: 0.4195 - val_accuracy: 0.8453 Epoch 53/100 115/115 [==============================] - 28s 247ms/step - loss: 0.3194 - accuracy: 0.8738 - val_loss: 0.4240 - val_accuracy: 0.8429 Epoch 54/100 115/115 [==============================] - 29s 248ms/step - loss: 0.3191 - accuracy: 0.8737 - val_loss: 0.4153 - val_accuracy: 0.8444 Epoch 55/100 115/115 [==============================] - 29s 248ms/step - loss: 0.3122 - accuracy: 0.8765 - val_loss: 0.4160 - val_accuracy: 0.8453 Epoch 56/100 115/115 [==============================] - 28s 248ms/step - loss: 0.3090 - accuracy: 0.8778 - val_loss: 0.4329 - val_accuracy: 0.8418 Epoch 57/100 115/115 [==============================] - 28s 246ms/step - loss: 0.3026 - accuracy: 0.8805 - val_loss: 0.4299 - val_accuracy: 0.8438 Epoch 58/100 115/115 [==============================] - 28s 247ms/step - loss: 0.2982 - accuracy: 0.8822 - val_loss: 0.4398 - val_accuracy: 0.8420 Epoch 59/100 115/115 [==============================] - 28s 248ms/step - loss: 0.2935 - accuracy: 0.8841 - val_loss: 0.4427 - val_accuracy: 0.8428 Epoch 60/100 115/115 [==============================] - 29s 248ms/step - loss: 0.2889 - accuracy: 0.8860 - val_loss: 0.4444 - val_accuracy: 0.8433 Epoch 61/100 115/115 [==============================] - 29s 248ms/step - loss: 0.2865 - accuracy: 0.8869 - val_loss: 0.4406 - val_accuracy: 0.8440 Epoch 62/100 115/115 [==============================] - 28s 247ms/step - loss: 0.2877 - accuracy: 0.8864 - val_loss: 0.4478 - val_accuracy: 0.8430 Epoch 63/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2862 - accuracy: 0.8868 - val_loss: 0.4562 - val_accuracy: 0.8403 Epoch 64/100 115/115 [==============================] - 29s 248ms/step - loss: 0.2876 - accuracy: 0.8862 - val_loss: 0.4692 - val_accuracy: 0.8363 Epoch 65/100 115/115 [==============================] - 28s 246ms/step - loss: 0.2876 - accuracy: 0.8862 - val_loss: 0.4775 - val_accuracy: 0.8334 Epoch 66/100 115/115 [==============================] - 29s 248ms/step - loss: 0.2938 - accuracy: 0.8839 - val_loss: 0.4722 - val_accuracy: 0.8358 Epoch 67/100 115/115 [==============================] - 29s 248ms/step - loss: 0.2890 - accuracy: 0.8857 - val_loss: 0.4617 - val_accuracy: 0.8401 Epoch 68/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2870 - accuracy: 0.8864 - val_loss: 0.4632 - val_accuracy: 0.8403 Epoch 69/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2973 - accuracy: 0.8820 - val_loss: 0.4520 - val_accuracy: 0.8420 Epoch 70/100 115/115 [==============================] - 29s 251ms/step - loss: 0.3033 - accuracy: 0.8800 - val_loss: 0.4585 - val_accuracy: 0.8381 Epoch 71/100 115/115 [==============================] - 29s 251ms/step - loss: 0.2982 - accuracy: 0.8818 - val_loss: 0.4585 - val_accuracy: 0.8368 Epoch 72/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2978 - accuracy: 0.8819 - val_loss: 0.4518 - val_accuracy: 0.8385 Epoch 73/100 115/115 [==============================] - 29s 248ms/step - loss: 0.3005 - accuracy: 0.8809 - val_loss: 0.4471 - val_accuracy: 0.8395 Epoch 74/100 115/115 [==============================] - 29s 248ms/step - loss: 0.2986 - accuracy: 0.8816 - val_loss: 0.4688 - val_accuracy: 0.8359 Epoch 75/100 115/115 [==============================] - 29s 251ms/step - loss: 0.2975 - accuracy: 0.8819 - val_loss: 0.4959 - val_accuracy: 0.8287 Epoch 76/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2983 - accuracy: 0.8816 - val_loss: 0.4673 - val_accuracy: 0.8337 Epoch 77/100 115/115 [==============================] - 29s 252ms/step - loss: 0.2962 - accuracy: 0.8824 - val_loss: 0.4520 - val_accuracy: 0.8373 Epoch 78/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2946 - accuracy: 0.8832 - val_loss: 0.4382 - val_accuracy: 0.8407 Epoch 79/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2996 - accuracy: 0.8809 - val_loss: 0.4475 - val_accuracy: 0.8394 Epoch 80/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2960 - accuracy: 0.8823 - val_loss: 0.4442 - val_accuracy: 0.8413 Epoch 81/100 115/115 [==============================] - 29s 251ms/step - loss: 0.2908 - accuracy: 0.8845 - val_loss: 0.4454 - val_accuracy: 0.8413 Epoch 82/100 115/115 [==============================] - 29s 252ms/step - loss: 0.2852 - accuracy: 0.8868 - val_loss: 0.4561 - val_accuracy: 0.8393 Epoch 83/100 115/115 [==============================] - 29s 251ms/step - loss: 0.2814 - accuracy: 0.8883 - val_loss: 0.4611 - val_accuracy: 0.8393 Epoch 84/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2771 - accuracy: 0.8901 - val_loss: 0.4543 - val_accuracy: 0.8414 Epoch 85/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2705 - accuracy: 0.8928 - val_loss: 0.4556 - val_accuracy: 0.8421 Epoch 86/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2655 - accuracy: 0.8947 - val_loss: 0.4655 - val_accuracy: 0.8417 Epoch 87/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2616 - accuracy: 0.8963 - val_loss: 0.4740 - val_accuracy: 0.8406 Epoch 88/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2596 - accuracy: 0.8972 - val_loss: 0.4760 - val_accuracy: 0.8401 Epoch 89/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2597 - accuracy: 0.8970 - val_loss: 0.4754 - val_accuracy: 0.8405 Epoch 90/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2607 - accuracy: 0.8966 - val_loss: 0.4785 - val_accuracy: 0.8394 Epoch 91/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2616 - accuracy: 0.8962 - val_loss: 0.4787 - val_accuracy: 0.8396 Epoch 92/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2627 - accuracy: 0.8957 - val_loss: 0.4803 - val_accuracy: 0.8402 Epoch 93/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2606 - accuracy: 0.8965 - val_loss: 0.4850 - val_accuracy: 0.8399 Epoch 94/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2614 - accuracy: 0.8961 - val_loss: 0.4850 - val_accuracy: 0.8385 Epoch 95/100 115/115 [==============================] - 29s 251ms/step - loss: 0.2587 - accuracy: 0.8972 - val_loss: 0.4751 - val_accuracy: 0.8377 Epoch 96/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2553 - accuracy: 0.8985 - val_loss: 0.4718 - val_accuracy: 0.8369 Epoch 97/100 115/115 [==============================] - 29s 249ms/step - loss: 0.2516 - accuracy: 0.9000 - val_loss: 0.4803 - val_accuracy: 0.8366 Epoch 98/100 115/115 [==============================] - 29s 248ms/step - loss: 0.2489 - accuracy: 0.9010 - val_loss: 0.4816 - val_accuracy: 0.8368 Epoch 99/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2475 - accuracy: 0.9015 - val_loss: 0.4844 - val_accuracy: 0.8367 Epoch 100/100 115/115 [==============================] - 29s 250ms/step - loss: 0.2519 - accuracy: 0.8997 - val_loss: 0.4804 - val_accuracy: 0.8355 . show_results(u_net, train, n=5) plt.suptitle(&quot;Test&quot;) plt.show() . show_results(u_net, test, n=5) plt.suptitle(&quot;Test&quot;) plt.show() .",
            "url": "https://jorgvt.github.io/Test_FastPages/2021/10/21/Segmentacion.html",
            "relUrl": "/2021/10/21/Segmentacion.html",
            "date": " • Oct 21, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://jorgvt.github.io/Test_FastPages/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://jorgvt.github.io/Test_FastPages/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://jorgvt.github.io/Test_FastPages/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://jorgvt.github.io/Test_FastPages/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}